<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="google-site-verification" content="na5zxdJgiLwCAa-YTdPyUVN6iYvuAQ4SQqi92fj4-9M" />

    <title>Matthew Riemer</title>

    <meta name="author" content="Matthew Riemer">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/jpg" href="images/MatthewRiemerPhoto.jpg">
    <style>
    .highlight-box {
      background-color: #eaf3fd;     /* light gray background */
      border: 1px solid #ddd;        /* light border */
      border-radius: 8px;            /* rounded corners */
      padding: 15px;                 /* spacing inside the box */
      margin-top: 10px;              /* optional spacing above */
      margin-bottom: 10px;           /* optional spacing below */
    }
  </style>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Matthew Riemer
                </p>
                <p>I work as a researcher in the <a href="https://research.ibm.com/artificial-intelligence">AI department</a> at the IBM T.J. Watson Research Center. I am also a final year PhD student at <a href="https://mila.quebec/en/">Mila, Quebec AI Institute</a> affiliated with <a href="https://www.umontreal.ca/">Université de Montréal</a> where I am supervised by <a href="https://www.irina-rish.com/">Irina Rish</a>.
                <p>
                  I am broadly interested in the question of how to build agents that learn efficiently over non-i.i.d. or non-stationary data distributions. This topic is often referred to as lifelong or continual learning. Over the course of my career, this has included research on many different aspects of this problem including: the role that meta-learning and modular architectures can have in finding a better balance between stability and plasticity, the formulation of the problem and correct objective for optimization, adapting in multi-agent environments where agents can change their policies, and learning adaptive policies with hierarchical reinforcement learning. If you are interested in working with me on related topics, please reach out by email!
                </p>
                <p style="text-align:center">
                  <a href="mailto:matthew.riemer@mila.quebec">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=PK7UzAwAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mattriemer/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/matthew-riemer-33459031/">LinkedIn</a> &nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/MatthewRiemerPhoto.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/MatthewRiemerPhoto.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%; border:0; border-spacing:0; border-collapse:separate; margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:20px; width:100%; vertical-align:middle">
                  <h2>Research Directions</h2>
                  <p>My primary ongoing research directions include:</p>
                  <div class="highlight-box">
                    <p>
                      <strong>Identifying Challenges in Optimizing the Stability-Plasticiy Dilemma:</strong> A key direction of my research has been advocating for formulating optimization of the stability-plasticity dilemma within the framework of reinforcement learning in continuing environments. Indeed, the key question we must consider to solve this dilemma is to what degree we expect certain information to be relevant in the long-term future. However, existing approaches are not able to learn efficiently in the presence of high mixing times, which we have shown to be pervasive for continual learning problems.
                    </p>
                    <ul>
                      <li><a href="https://arxiv.org/pdf/2012.13490">Towards Continual Reinforcement Learning: A Review and Perspectives</a> [JAIR 2022]</li>
                      <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/89c61fce5a8b73871d1c4073f486b134-Paper-Conference.pdf">Continual Learning in Environments with Polynomial Mixing Times</a> [NeurIPS 2022]</li>
                      <li><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/92f79f493ca2d6c0ba04c3af76bb3368-Paper-Conference.pdf">Balancing Context Length and Mixing Times for Reinforcement Learning at Scale</a> [NeurIPS 2024]</li>
                    </ul>
                  </div>
                  <div class="highlight-box">
                    <p>
                      <strong>Learning to Continually Learn:</strong> My most influential work to date has been the invention of the "meta-experience replay" technique, which combines experience replay with a meta-learning process in which the agent learns to align gradients across continual updates to improve both stability and plasticity when training neural networks. Our recent study further demonstrates that this technique still remains a state-of-the-art approach today that improves on the performance of experience replay for continual pre-training of LLMs while incurring marginal compute overhead.
                    </p>
                    <ul>
                      <li><a href="https://arxiv.org/pdf/1810.11910">Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference</a> [ICLR 2019]</li>
                      <li><a href="https://arxiv.org/pdf/2508.01908">Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models</a> [CoLLAs 2025]</li>
                    </ul>
                  </div>
                  <div class="highlight-box">
                    <p>
                      <strong>Dynamic Multi-Agent Interaction:</strong> The truth is that the world is largely stationary in the sense that the laws of physics are stationary. When we say that the world is changing, we are generally referring to changes in behavior among agents. As such, I view the study of how to adapt to other agents as they learn as a key aspect of the continual learning problem.
                    </p>
                    <ul>
                      <li><a href="https://arxiv.org/pdf/1805.07830">Learning to Teach in Cooperative Multiagent Reinforcement Learning</a> [AAAI 2019]</li>
                      <ul>
                      <li><em><a href="https://aaai.org/about-aaai/aaai-awards/aaai-conference-paper-awards-and-recognition/"> Awarded Outstanding Student Paper Honorable Mention </a></em></li>
                      </ul>
                      <li><a href="https://arxiv.org/pdf/1903.03216">Learning Hierarchical Teaching Policies for Cooperative Agents</a> [AAMAS 2020]</li>
                      <li><a href="https://proceedings.mlr.press/v139/kim21g/kim21g.pdf">A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning</a> [ICML 2021]</li>
                      <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/7749f9c0d5ff109231be21e910a3ced2-Paper-Conference.pdf">Influencing Long-Term Behavior in Multiagent Reinforcement Learning</a> [NeurIPS 2022]</li>
                      <li><a href="https://arxiv.org/pdf/2412.19726">Position: Theory of Mind Benchmarks are Broken for Large Language Models</a> [ICML 2025]</li>
                    </ul>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Papers</h2>
                <p>
                  Here is a collection of previous research papers I have collaborated on:
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
          <img src="images/retained_loss_v_flops3.png" alt="rtb" height="250" width="400">
        </td>
        <td width="75%" valign="middle">
            <papertitle><a href="https://arxiv.org/pdf/2508.01908">Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models</a></papertitle>
          <br><p>
            Istabrak Abbes, Gopeshh Subbaraj, <strong>Matthew Riemer</strong>, Nizar Islah, Benjamin Therien, Tsuguchika Tabaru, Hiroaki Kingetsu, Sarath Chandar, and Irina Rish
          <br>
          <em>Conference on Lifelong Learning Agents (CoLLAs) 2025</em>
          <br>
        </p>
          <p>
          <a href="https://arxiv.org/pdf/2508.01908">Paper</a> | <a href="https://github.com/chandar-lab/continual-pretraining">Code</a>
          </p>
        </td>
      </tr>
            
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
          <img src="images/adaptivestack_loop.png" alt="rtb" height="250" width="400">
        </td>
        <td width="75%" valign="middle">
            <papertitle><a href="https://openreview.net/pdf?id=W7O95Q1qzT">Finding the FrameStack: Learning What to Remember for Non-Markovian Reinforcement Learning</a></papertitle>
          <br><p>
           Geraud Nangue Tasse, <strong>Matthew Riemer</strong>, Benjamin Rosman, and Tim Klinger
          <br>
          <em>Reinforcement Learning Conference (RLC) 2025 Finding the Frame Workshop</em>
          <br>
          </p>
          <p>
          <a href="https://openreview.net/pdf?id=W7O95Q1qzT">Paper</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
          <img src="images/alignment.png" alt="rtb" height="250" width="400">
        </td>
        <td width="75%" valign="middle">
            <papertitle><a href="https://aclanthology.org/2025.acl-short.22.pdf">Combining Domain and Alignment Vectors Provides Better Knowledge-Safety Trade-offs in LLMs</a></papertitle>
          <br><p>
           Megh Thakkar, Quentin Fournier, <strong>Matthew Riemer</strong>, Pin-Yu Chen, Amal Zouaq, Payel Das, and Sarath Chandar
          <br>
          <em>Association for Computational Linguistics (ACL) 2025</em>
          <br>
          </p>
          <p>
          <a href="https://aclanthology.org/2025.acl-short.22.pdf">Paper</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
          <img src="images/epman.png" alt="rtb" height="250" width="400">
        </td>
        <td width="75%" valign="middle">
            <papertitle><a href="https://aclanthology.org/2025.acl-long.574.pdf">EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts</a></papertitle>
          <br><p>
           Subhajit Chaudhury, Payel Das, Sarath Swaminathan, Georgios Kollias, Elliot Nelson, Khushbu Pahwa, Tejaswini Pedapati, Igor Melnyk, and <strong>Matthew Riemer</strong>
          <br>
          <em>Association for Computational Linguistics (ACL) 2025</em>
          <br>
          </p>
          <p>
          <a href="https://aclanthology.org/2025.acl-long.574.pdf">Paper</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
          <img src="images/rps.jpeg" alt="rtb" height="250" width="400">
        </td>
        <td width="75%" valign="middle">
            <papertitle><a href="https://arxiv.org/pdf/2412.19726">Position: Theory of Mind Benchmarks are Broken for Large Language Models</a></papertitle>
          <br><p>
           <strong>Matthew Riemer</strong>, Zahra Ashktorab, Djallel Bouneffouf, Payel Das, Miao Liu, Justin D Weisz, and Murray Campbell
          <br>
          <em>International Conference on Machine Learning (ICML) 2025</em>
          <br>
          </p>
          <p>
          <a href="https://arxiv.org/pdf/2412.19726">Paper</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
          <img src="images/asynchronous_interaction.png" alt="rtb" height="250" width="400">
        </td>
        <td width="75%" valign="middle">
            <papertitle><a href="https://openreview.net/pdf?id=fXb9BbuyAD">Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference</a></papertitle>
          <br><p>
           <strong>Matthew Riemer</strong>, Gopeshh Subbaraj, Glen Berseth, and Irina Rish
          <br>
          <em>International Conference on Learning Representations (ICLR) 2025</em>
          <br>
          </p>
          <p>
          <a href="https://openreview.net/pdf?id=fXb9BbuyAD">Paper</a> | <a href="https://github.com/CERC-AAI/realtime_rl">Code</a> | <a href="https://mila.quebec/en/article/real-time-reinforcement-learning">Blog</a>
          </p>
        </td>
      </tr>

        </td>
      </tr>
    </table>
  </body>
</html>
