<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="google-site-verification" content="na5zxdJgiLwCAa-YTdPyUVN6iYvuAQ4SQqi92fj4-9M" />

    <title>Matthew Riemer</title>

    <meta name="author" content="Matthew Riemer">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/jpg" href="images/MatthewRiemerPhoto.jpg">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Matthew Riemer
                </p>
                <p>I work as a researcher in the <a href="https://research.ibm.com/artificial-intelligence">AI department</a> at the IBM T.J. Watson Research Center. I am also a final year PhD at <a href="https://mila.quebec/en/">Mila, Quebec AI Institute</a> affiliated with <a href="https://www.umontreal.ca/">Université de Montréal</a> where I am supervised by <a href="https://www.irina-rish.com/">Irina Rish</a>.
                <p>
                  I am broadly interested in the question of how to build agents that learn efficiently over non-i.i.d. or non-stationary data distributions. This topics is often referred to as lifelong or continual learning. Over the course of my career, this has included research on many different aspects of this problem including: the role that meta-learning and modular architectures can have in finding a better balance between stability and plasticity, the formulation of the problem and correct objective for optimization, adapting in multi-agent environments where agents can change their policies, and learning adaptive policies with hierarchical reinforcement learning. If you are interested in working with me on related topics, please reach out by email!
                </p>
                <p style="text-align:center">
                  <a href="mailto:matthew.riemer@mila.quebec">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=PK7UzAwAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mattriemer/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/matthew-riemer-33459031/">LinkedIn</a> &nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/MatthewRiemerPhoto.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/MatthewRiemerPhoto.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  Here is a collection of my previous research.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
          <img src="images/retained_loss_v_flops3.png" alt="rtb" height="160" width="400">
        </td>
        <td width="75%" valign="middle">
            <papertitle><a href="https://arxiv.org/pdf/2508.01908">Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models</a></papertitle>
          <br><p>
            Istabrak Abbes, Gopeshh Subbaraj, <strong>Matthew Riemer</strong>, Nizar Islah, Benjamin Therien, Tsuguchika Tabaru, Hiroaki Kingetsu, Sarath Chandar, and Irina Rish
          <br>
          <em>Conference on Lifelong Learning Agents (CoLLAs) 2025</em>
          <br>
        </p>
        </td>
      </tr>
            
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
          <img src="images/adaptivestack_loop.png" alt="rtb" height="160" width="400">
        </td>
        <td width="75%" valign="middle">
            <papertitle><a href="https://openreview.net/pdf?id=W7O95Q1qzT">Finding the FrameStack: Learning What to Remember for Non-Markovian Reinforcement Learning</a></papertitle>
          <br><p>
           Geraud Nangue Tasse, <strong>Matthew Riemer</strong>, Benjamin Rosman, and Tim Klinger
          <br>
          <em>Reinforcement Learning Conference (RLC) 2025 Finding the Frame Workshop</em>
          <br>
        </p>
        </td>
      </tr>

        </td>
      </tr>
    </table>
  </body>
</html>
